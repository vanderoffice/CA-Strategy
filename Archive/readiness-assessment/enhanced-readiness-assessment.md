# California Technology Readiness Assessment Form
## Enhanced Version with Detailed Scoring Examples

### Instructions for Completing This Assessment

This assessment measures your department's organizational readiness to successfully execute technology modernization projects. The assessment evaluates six critical domains that research and experience have shown are essential for modernization success.

**How to Use This Form:**
- Review each indicator carefully and assess your department's current state honestly
- For each indicator, select a score from 1 (lowest/initial) to 5 (highest/optimized) 
- Use the detailed examples provided for scores 1, 3, and 5 as anchors to guide your rating
- Scores of 2 and 4 represent intermediate states between the anchor points
- Provide specific comments and evidence to support your rating
- When uncertain between two scores, choose the more conservative (lower) rating

**Assessment Team:**
This assessment should be completed collaboratively by your department's cross-functional leadership team, including:
- Chief Information Officer (CIO) or IT Director
- Chief Financial Officer (CFO) or Budget Officer
- Chief Information Security Officer (CISO) or Information Security Officer (ISO)
- Human Resources Director or Workforce Development Lead
- Project Management Office (PMO) Director (if applicable)
- Strategic Planning Lead or Chief of Staff
- Department Director or Executive Sponsor for final validation

**Scoring Framework:**
- **Score 1 (Initial/Ad-hoc):** Little to no formal capability; reactive, inconsistent approach
- **Score 2 (Emerging):** Some awareness and initial efforts, but sporadic and undocumented
- **Score 3 (Developing):** Formal processes established but not fully mature or consistently applied
- **Score 4 (Managed):** Well-documented, consistent practices that are widely understood and followed
- **Score 5 (Optimized):** Mature, continuously improving practices that serve as best-in-class models

---

## DOMAIN 1: Leadership and Governance

*This domain assesses the presence and quality of executive leadership commitment, strategic planning, and governance structures necessary to sustain modernization initiatives through completion and beyond.*

### 1.1 Executive Sponsorship and Engagement

**Why This Matters:** Executive sponsorship is the single most critical success factor for technology modernization. Active executive champions secure resources, remove organizational barriers, make timely decisions, and signal organizational priority.

**Rating Scale and Examples:**

**Score 1 - No Formal Sponsorship:**
- Technology initiatives are driven solely by IT staff without executive involvement
- Executives view technology as an IT department concern, not a strategic priority
- No executive has been formally assigned responsibility for modernization efforts
- Technology projects compete for attention but receive minimal leadership focus
- Example: "Our IT Director manages all technology projects independently. Department leadership is briefed occasionally but doesn't actively participate in technology decisions or planning."

**Score 3 - Passive Sponsorship:**
- An executive sponsor has been formally designated for technology initiatives
- Sponsor attends periodic steering committee meetings but delegates most decisions
- Sponsor provides approval when asked but doesn't proactively drive initiatives
- Some resources are allocated, but competing priorities often take precedence
- Barriers are escalated to the sponsor, but resolution can be slow or inconsistent
- Example: "Our Deputy Director is the assigned sponsor for our case management modernization project. They attend monthly steering meetings and approve budget requests, but the IT Director handles day-to-day decisions and problem-solving."

**Score 5 - Active Championship:**
- Executive sponsor is visibly and consistently engaged in modernization efforts
- Sponsor proactively communicates the strategic importance of technology to the organization
- Sponsor personally removes barriers, secures funding, and facilitates cross-departmental cooperation
- Sponsor makes timely decisions and holds project teams accountable for delivery
- Sponsor's active involvement signals organizational priority and commitment
- Example: "Our Department Director chairs our Technology Steering Committee, meets weekly with the modernization project team, presents technology updates at executive staff meetings, personally resolved our staffing challenges with HR and Budget, and regularly communicates modernization progress to all staff."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 1.2 IT Strategic Alignment

**Why This Matters:** Technology modernization must align with and support the department's mission, strategic priorities, and business objectives. Without this alignment, technology investments become disconnected from organizational needs and fail to deliver meaningful value.

**Rating Scale and Examples:**

**Score 1 - Reactive/Ad-hoc (No Documented Strategy):**
- No documented IT strategic plan exists
- Technology decisions are made reactively in response to immediate problems or crises
- IT investments are disconnected from department mission and strategic goals
- No formal process links business needs to technology capabilities
- Technology planning happens independently from department strategic planning
- Example: "We don't have an IT strategic plan. Our IT team responds to system failures and user requests as they arise. Technology purchases are made when budgets allow, based on immediate needs rather than a coordinated strategy."

**Score 3 - Plan Exists, But Misaligned or Outdated:**
- A formal IT strategic plan has been developed but is not regularly updated
- The plan exists as a standalone document with limited integration into department strategic planning
- Some alignment between IT priorities and business needs exists, but gaps are evident
- Plan is referenced occasionally but doesn't drive day-to-day technology decisions
- Technology governance may not consistently use the plan to prioritize investments
- Example: "We developed an IT Strategic Plan in 2022 that identified cloud migration and data modernization as priorities. However, the plan hasn't been updated since then, and several major technology decisions have been made outside the plan's framework. Our department strategic plan mentions technology but doesn't reference the IT plan specifically."

**Score 5 - Fully Aligned and Integrated Strategy:**
- Current, documented IT strategic plan is fully integrated with department strategic plan
- Technology priorities directly support and enable mission-critical business objectives
- Joint planning process ensures business leaders and IT collaborate on strategy
- Technology investments are explicitly mapped to strategic goals and outcomes
- Plan is actively used to guide technology governance decisions and resource allocation
- IT strategy is reviewed and updated annually in conjunction with business planning
- Example: "Our IT Strategic Plan is updated annually as part of our department strategic planning cycle. Each technology initiative is explicitly linked to department strategic goals. Our CIO participates in executive strategic planning, and our Technology Steering Committee uses the strategic plan to prioritize all technology investments. Recent examples include our permit automation system (supporting Goal 2: Reduce processing times) and cloud data platform (supporting Goal 4: Data-driven decision making)."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 1.3 Governance and Oversight

**Why This Matters:** Effective governance structures ensure appropriate oversight, timely decision-making, stakeholder engagement, and accountability for technology investments. Strong governance prevents project drift, manages risks, and ensures resources are used effectively.

**Rating Scale and Examples:**

**Score 1 - Informal/Ad-hoc Decision-Making:**
- No formal IT governance body or structure exists
- Technology decisions are made by individuals without consistent process or criteria
- Stakeholders are not systematically engaged in technology planning or oversight
- No regular forum exists for technology investment review and prioritization
- Decision-making authority and accountability are unclear
- Example: "Technology decisions are made by our IT Manager based on available budget and urgent needs. There's no formal committee or governance process. Major technology purchases are approved through our standard procurement process, but there's no technology-specific oversight or prioritization mechanism."

**Score 3 - Committee Exists But Inconsistent:**
- A technology governance committee or steering group has been established
- Committee meets periodically but not on a consistent schedule
- Committee membership is defined but attendance and engagement vary
- Some formal processes exist for decision-making but aren't always followed
- Decision authority is defined but may not be consistently exercised
- Committee reviews major projects but may lack authority to make binding decisions
- Example: "We established an IT Steering Committee three years ago with representation from each division plus our CIO and CFO. The committee is supposed to meet quarterly but meetings are sometimes postponed. We review major technology projects and make recommendations, but final decisions are often made outside the committee. We have governance charters and decision frameworks, but they're not always consistently applied."

**Score 5 - Mature, Transparent, and Empowered Governance:**
- Formal, well-documented governance structure with clear roles and decision rights
- Governance body meets regularly on a published schedule with consistent participation
- Membership includes appropriate business and technology leadership with decision authority
- Transparent processes for project prioritization, investment decisions, and risk management
- Governance body has real authority and accountability for technology portfolio
- Regular communication of governance decisions to broader organization
- Governance processes are documented, followed consistently, and continuously improved
- Example: "Our Technology Governance Board includes our Deputy Director (Chair), CIO, CFO, CISO, and division chiefs. We meet monthly with published agendas and minutes. The Board approves all technology investments over $50K, reviews project health dashboards, manages the technology portfolio, and has authority to reallocate resources or halt underperforming projects. Our governance charter clearly defines decision authority, escalation processes, and accountability. We publish quarterly governance reports to all staff and conduct annual governance process reviews for improvement."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

## DOMAIN 2: Data and Technology Infrastructure

*This domain evaluates the technical foundation necessary to support modernization, including cloud adoption, system interoperability, and data management capabilities.*

### 2.1 Cloud Adoption and Legacy System Dependency

**Why This Matters:** Cloud technologies enable scalability, flexibility, security, and cost efficiency. Heavy dependence on aging legacy systems creates technical debt, limits innovation, increases security risks, and makes modernization more difficult and expensive.

**Rating Scale and Examples:**

**Score 1 - All Critical Systems On-Premises/Legacy, No Cloud Strategy:**
- All mission-critical systems run on on-premises infrastructure or mainframes
- No cloud services are currently in use for production workloads
- No documented strategy or plan for cloud adoption
- Limited understanding of cloud capabilities and benefits within IT and leadership
- Significant technical debt in legacy systems (aging hardware, unsupported software)
- No budget allocated for cloud migration or exploration
- Example: "All our systems run in our on-site data center, including our 20-year-old mainframe that handles licensing and our AS/400 for financial systems. We have basic email in the cloud, but no production applications. We haven't developed a cloud strategy because of concerns about security and the complexity of migrating our legacy systems."

**Score 3 - Some Non-Critical Workloads Shifted, No Comprehensive Strategy:**
- Email, collaboration tools, or other productivity applications have moved to cloud (Microsoft 365, Google Workspace)
- Some non-critical applications or development/test environments utilize cloud infrastructure
- Cloud use is opportunistic rather than strategic (individual projects, not enterprise approach)
- No comprehensive cloud adoption strategy or enterprise architecture for cloud
- Mission-critical systems remain on-premises without migration plans
- Limited cloud expertise within IT staff
- Example: "We migrated to Microsoft 365 three years ago for email and collaboration. Our web development team uses Azure for dev/test environments. A few newer applications run in AWS, but these were individual project decisions. Our core systems (permit system, licensing database, financial system) remain in our data center. We haven't developed an enterprise cloud strategy, though we're exploring options."

**Score 5 - Cloud Smart Strategy, Actively Migrating Legacy Systems:**
- Documented, approved cloud adoption strategy aligned with state Cloud Smart policy
- Clear roadmap for migrating appropriate workloads to cloud (cloud-first, not cloud-only approach)
- Active migration projects in progress for legacy systems based on business value and technical feasibility
- Hybrid cloud architecture supports both cloud and on-premises systems during transition
- IT staff trained in cloud technologies; cloud expertise exists in-house or through partners
- Cloud governance processes ensure security, cost management, and compliance
- Data center footprint actively shrinking as workloads migrate
- Example: "Our Cloud Adoption Strategy, approved by our governance board in 2024, prioritizes migration of 15 applications over three years. We've successfully migrated our permit tracking system and document management to Azure Gov Cloud. Our licensing system is currently being refactored for cloud deployment. We maintain a hybrid environment managed through our enterprise architecture framework. Our IT staff have completed cloud certification training, and we have embedded cloud architects. We track cloud costs, security, and optimization through our governance process."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 2.2 Interoperability and API Readiness

**Why This Matters:** Modern digital government requires systems to share data and integrate seamlessly. API-enabled architectures allow systems to communicate efficiently, reduce manual data entry, improve data quality, and enable innovative service delivery models.

**Rating Scale and Examples:**

**Score 1 - Siloed Systems with Manual Data Sharing:**
- Systems operate independently with no automated data exchange
- Data sharing between systems occurs through manual processes (printing, re-keying, emailing spreadsheets)
- No APIs exist to enable programmatic data access or integration
- System integration is not considered in technology decisions or procurements
- Significant data redundancy and quality issues due to manual re-entry
- Example: "Each division has its own database system. When we need to share data, staff print reports or export to Excel and email files. For example, our licensing data must be manually entered into our enforcement tracking system. We have no APIs or automated integration between systems. New system procurements focus on departmental needs without consideration of enterprise integration."

**Score 3 - Point-to-Point Integrations, Some APIs Emerging:**
- Some systems are connected through custom point-to-point integrations (file transfers, database connections)
- A few systems offer APIs, but use is limited and not standardized
- Integration approaches are inconsistent; each integration is custom-developed
- No enterprise integration platform or API management infrastructure
- Integration is addressed on a case-by-case basis when critical needs arise
- Limited documentation of existing integrations and APIs
- Example: "We have automated several critical data exchanges: nightly file transfers between our licensing and billing systems, a database link between our case management and document systems. Two of our newer applications have REST APIs that we use for internal integration. However, we don't have a standard approach to integration. Each integration was custom-built for specific needs. We have no API catalog or governance process."

**Score 5 - Enterprise API Platform with Reusable Services:**
- Enterprise API management platform implemented (e.g., Apigee, MuleSoft, Azure API Management)
- Reusable API services catalog documents available APIs and integration patterns
- Modern integration architecture uses API-first design principles
- APIs secured with authentication, authorization, and monitoring
- API governance process ensures consistency, documentation, and lifecycle management
- Systems procurement requires API capabilities and integration readiness
- Internal and external stakeholders can discover and consume APIs
- Example: "We implemented Azure API Management in 2023 as our enterprise integration platform. Our API catalog includes 27 documented APIs covering licensing, permits, inspections, and reporting services. All new systems must provide RESTful APIs per our enterprise architecture standards. APIs are secured with OAuth 2.0 and monitored for performance and usage. Our API governance process includes design reviews, documentation requirements, and versioning standards. We recently enabled our data partners to access permit data through our public API portal, reducing manual data requests by 75%."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 2.3 Data Strategy and Governance

**Why This Matters:** Data is a strategic asset. Effective data governance ensures data quality, security, privacy, and accessibility. Without data governance, modernization efforts struggle with data quality issues, compliance risks, and inability to leverage data for decision-making.

**Rating Scale and Examples:**

**Score 1 - No Governance or Quality Processes:**
- No formal data governance program or policies exist
- No designated data stewards or data governance roles
- Data quality issues are widespread but not systematically addressed
- No data catalog or inventory of department data assets
- Data definitions are inconsistent across systems
- No formal processes for data access requests or data sharing agreements
- Example: "We don't have a formal data governance program. Data management is handled individually by each system administrator. We frequently encounter data quality problems—duplicate records, missing data, inconsistent values—but address them reactively. We don't have a comprehensive understanding of what data assets we have or where they're located. Data access is managed informally through individual requests to IT staff."

**Score 3 - Draft Policies Exist, Roles Emerging:**
- Data governance policy has been drafted but not fully implemented
- Data governance roles have been defined (Chief Data Officer, data stewards) but are not fully operational
- Data quality issues are recognized, and some quality improvement efforts are underway
- Initial data inventory or catalog effort has begun but is incomplete
- Some standardized data definitions exist for critical data elements
- Data governance committee has been formed but meets irregularly
- Example: "We established a Data Governance Committee in 2023 and drafted data governance policies that are under executive review. We designated data stewards for each major program area, but they're performing these duties in addition to their regular responsibilities. We've begun creating a data inventory and have documented data dictionaries for our two largest systems. The committee meets quarterly to discuss data quality issues and cross-program data needs."

**Score 5 - Active Program with Data Catalog and Stewards:**
- Comprehensive, approved data governance program with executive sponsorship
- Chief Data Officer or equivalent with dedicated authority and resources
- Active data stewards assigned for all major data domains with clear responsibilities
- Enterprise data catalog documents all major data assets, definitions, and metadata
- Data quality metrics are measured, monitored, and acted upon
- Formal processes for data access, sharing, privacy, and security
- Data governance integrated into technology project lifecycle and procurement
- Regular training on data responsibilities and data literacy for staff
- Example: "Our Chief Data Officer leads our Data Governance Program, established in 2022 with executive sponsorship and dedicated FTE resources. We have 12 assigned data stewards representing all program areas who meet monthly. Our enterprise data catalog (built on Microsoft Purview) documents 150+ data assets with standardized metadata. Data quality dashboards track completeness, accuracy, and timeliness metrics with improvement goals. All technology projects must complete data governance review including privacy impact assessments. We provide quarterly data literacy training to staff and maintain data governance policies that are reviewed annually."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

## DOMAIN 3: Organizational Culture and Workforce Capability

*This domain examines the human and cultural dimensions of readiness: staff openness to change, digital skills, and the presence of modern technology roles necessary to sustain innovation.*

### 3.1 Openness to Change and Collaboration

**Why This Matters:** Technology modernization requires organizational change. Departments with cultures that embrace change, encourage innovation, and collaborate across organizational boundaries are far more likely to successfully adopt new technologies and processes.

**Rating Scale and Examples:**

**Score 1 - High Resistance and Skepticism:**
- Staff and leadership express significant resistance to technology changes
- Previous technology initiatives met with skepticism or active opposition
- "We've always done it this way" mentality prevails
- Divisions operate in silos with limited cross-functional collaboration
- Change initiatives are often abandoned when faced with resistance
- Innovation is discouraged; staff fear proposing new approaches
- Example: "Our last attempt to implement a new case management system was abandoned after six months due to staff resistance. Program staff were skeptical the new system would work and preferred their existing processes. Divisions operate independently and rarely collaborate on shared challenges. Staff are hesitant to suggest process improvements, fearing criticism. Management often retreats from change initiatives when they encounter pushback."

**Score 3 - Pockets of Willingness, Inconsistent Across Department:**
- Some individuals and teams are open to change; others remain resistant
- Support for change varies significantly by division or leadership
- Previous change initiatives had mixed success; some succeeded, others failed
- Some cross-functional collaboration occurs, but not systematically
- Staff participation in change efforts is voluntary; engagement varies
- Innovation is tolerated but not actively encouraged or resourced
- Example: "Our Operations Division successfully implemented a new mobile inspection system, with staff enthusiastically adopting the technology. However, our Licensing Division resisted a similar effort and continues using paper-based processes. We have a few cross-functional workgroups that collaborate effectively, but most divisions operate independently. Some managers encourage staff ideas; others prefer to maintain existing approaches. Staff engagement in change initiatives depends heavily on the specific manager and division culture."

**Score 5 - Change-Ready Culture, Staff Engaged Early:**
- Department culture actively embraces continuous improvement and innovation
- Staff at all levels are encouraged to identify improvement opportunities
- Change management is integrated into all major initiatives from the start
- Cross-functional collaboration is standard practice; teams work across organizational boundaries
- Staff are engaged early in technology and process changes; their input shapes solutions
- Previous modernization efforts succeeded due to strong staff engagement
- Innovation and experimentation are resourced and celebrated
- Example: "Our department has established a culture of continuous improvement. We have an active Innovation Program where staff can propose process and technology improvements, with 15 staff-initiated changes implemented in the past year. When we began our permit system modernization, we formed a cross-functional design team with representation from all divisions. Staff participated in user research, prototype testing, and training design. Our change management approach includes early stakeholder engagement, transparent communication, and champions network. Recent surveys show 82% of staff feel positive about technology changes and believe their input matters."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 3.2 Digital Literacy and Skills

**Why This Matters:** Staff must possess digital skills to effectively use modern technology systems. Low digital literacy creates training burden, reduces system adoption, limits return on investment, and frustrates both staff and technology initiatives.

**Rating Scale and Examples:**

**Score 1 - Skills Limited to Basic Office Tools:**
- Most staff proficient only with basic word processing and email
- Significant portion of workforce struggles with standard productivity tools
- Limited understanding of modern technologies (cloud, mobile, collaboration platforms)
- Staff require extensive hand-holding for any new technology
- Resistance to technology stems partly from discomfort and lack of skills
- No formal digital literacy training program exists
- Example: "Our workforce has basic email and Word skills, but many staff struggle with Excel, collaboration tools, or any technology beyond their primary application. When we rolled out Microsoft Teams, adoption was very slow because staff found it confusing. We don't have a technology training program beyond occasional vendor-provided training for specific systems. Our IT help desk spends significant time on basic how-to questions."

**Score 3 - Training Offered, Adoption Uneven:**
- Formal technology training is available but participation is optional
- Digital literacy varies widely across the organization
- Some staff are highly proficient; others continue to struggle
- Training focuses on specific tools rather than building broad digital competency
- Technology adoption and skill development depend on individual initiative
- No systematic assessment of digital skill levels or gaps
- Example: "We offer quarterly training sessions on our major systems and productivity tools. About 40% of staff participate in optional training sessions. Some teams have become quite tech-savvy and use advanced features of our systems, while others continue to use work-arounds or request IT help for basic tasks. We haven't assessed department-wide digital literacy levels or created personalized learning paths. Training is generic rather than role-based or skill-level-appropriate."

**Score 5 - Continuous Learning Culture with Data and Tool Proficiency:**
- Comprehensive digital literacy program with mandatory baseline training for all staff
- Role-based training paths ensure staff develop skills needed for their responsibilities
- Learning management system tracks training completion and skill development
- Regular assessment of digital skills identifies gaps and training needs
- Advanced training available for staff to develop expertise (data analysis, process automation, etc.)
- Staff demonstrate strong proficiency with department tools and modern platforms
- Technology training integrated into onboarding and career development
- Example: "All staff complete a digital literacy baseline training within 30 days of hire, covering productivity tools, cybersecurity, data handling, and collaboration platforms. We offer role-based learning paths in our LMS (LinkedIn Learning integrated with our HR system) with recommended courses for each position type. Annual skill assessments identify individual and team training needs. We offer advanced training in data analysis, process automation, and citizen engagement for interested staff. 90% of staff have completed required training, and our recent system implementations achieved 95%+ adoption rates within 60 days. Staff regularly use data dashboards, collaborative workspaces, and mobile tools without requiring IT support."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 3.3 Modern Technology Roles and Training

**Why This Matters:** Modern technology requires specialized roles beyond traditional IT support: cloud architects, data engineers, DevOps specialists, UX designers, cybersecurity analysts, and agile delivery teams. Without these capabilities, departments cannot effectively modernize or sustain modern systems.

**Rating Scale and Examples:**

**Score 1 - Traditional IT Roles Only:**
- IT organization consists solely of traditional roles: system administrators, help desk, network technicians
- No specialized roles for cloud, security, data, or user experience
- IT staff skill sets focused on maintaining legacy on-premises systems
- No formal training or career development program for IT staff
- Difficult to recruit or retain staff with modern technology skills
- All specialized expertise must be obtained through vendors or consultants
- Example: "Our IT organization includes a Director, three system administrators, two help desk technicians, and a network administrator. All our staff are focused on maintaining our on-premises infrastructure and supporting current systems. We don't have cloud architects, security specialists, or data engineers. When we need specialized expertise for projects, we must hire consultants. We've had difficulty recruiting IT staff because our technology environment is perceived as outdated and we can't offer competitive salaries for specialized roles."

**Score 3 - Some Modern Roles Emerging:**
- A few specialized positions have been created (e.g., cybersecurity analyst, web developer)
- Modern roles exist but may be underfilled or filled with staff learning on the job
- Some staff training on modern technologies provided, but not comprehensive
- Mixture of traditional and modern capabilities; legacy focus remains dominant
- Challenges remain in recruiting and retaining specialized talent
- Limited career progression paths for modern technology roles
- Example: "We created an Information Security Officer position two years ago and hired an experienced cybersecurity professional. We have one cloud administrator who supports our Microsoft 365 environment and is learning Azure. Our application development team (two staff) has begun learning agile methodologies. However, most of our IT organization remains focused on traditional infrastructure support. We sent several staff to cloud training courses, but we lack clear career paths for specialized roles. We're still heavily dependent on contractors for cloud architecture and specialized development work."

**Score 5 - Agile Teams, Modern Roles, and Ongoing Training:**
- IT organization includes full complement of modern roles: cloud architects, DevOps engineers, data engineers, security analysts, UX designers
- Positions are filled with qualified professionals; competitive compensation and career paths
- Teams organized around products and outcomes rather than traditional IT functions
- Agile, DevOps, and user-centered design practices are standard operating procedures
- Comprehensive professional development program keeps staff current with evolving technologies
- Strong recruitment pipeline and competitive retention rates for specialized roles
- Strategic use of staff augmentation to fill temporary gaps or specialized needs
- Example: "Our IT organization has been transformed over the past three years. We reorganized into cross-functional product teams, each including developers, cloud engineers, data specialists, security, and UX. We created and filled cloud architect, DevOps engineer, data engineer, and UX designer positions. Our teams practice agile delivery with two-week sprints and continuous deployment. All IT staff receive $3,000 annual professional development budgets and time for learning. We implemented competitive salary ranges and clear career progression for specialized roles, improving our retention rate to 92%. We strategically use contractors to supplement teams during peak demand while maintaining core capabilities in-house."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

## DOMAIN 4: Cybersecurity and Risk Management

*This domain assesses compliance with state security mandates and the maturity of security governance and risk management practices essential for protecting technology investments and public data.*

### 4.1 State Policy Compliance (SAM/SIMM 5300)

**Why This Matters:** Compliance with State Administrative Manual (SAM) and Statewide Information Management Manual (SIMM) Section 5300 security requirements is mandatory for all state departments. Non-compliance creates legal risk, exposes systems to threats, and can block project approvals and funding.

**Rating Scale and Examples:**

**Score 1 - Non-Compliant with No Documentation:**
- No documented information security program exists
- Unaware of or not following SAM/SIMM 5300 requirements
- No security policies, procedures, or standards documented
- Security controls are ad-hoc or absent
- No security assessments or audits have been conducted
- No designated Information Security Officer (ISO)
- Significant security vulnerabilities likely exist but are unknown
- Example: "We don't have a formal information security program or documented security policies. Our IT staff implement security measures based on their judgment and experience, but we haven't documented our approach. We have not conducted a formal assessment against SIMM 5300 requirements and don't have a designated ISO. We're unsure whether our practices align with state security requirements. No external security audits have been performed."

**Score 3 - Partial Compliance with Identified Gaps:**
- Information Security Officer (ISO) has been designated
- Some security policies and procedures documented, but incomplete
- Aware of SAM/SIMM 5300 requirements; working toward compliance
- Security assessment conducted; gaps identified but remediation in progress
- Basic security controls implemented (firewalls, antivirus, access controls) but not comprehensive
- Security program exists but lacks resources or organizational support for full compliance
- Example: "We designated an ISO two years ago (50% time allocation) who has been working to establish our security program. We've documented security policies for access control, incident response, and acceptable use. We conducted a SIMM 5300 gap assessment last year that identified 23 gaps across various control areas. We've addressed high-priority items (multi-factor authentication, vulnerability scanning) but resource constraints have slowed progress on remaining gaps. Our security controls are stronger than previously but not fully compliant. We're working with CDT on a remediation roadmap."

**Score 5 - Fully Compliant, Audited, and Integrated:**
- Comprehensive information security program fully aligned with SAM/SIMM 5300
- Dedicated Information Security Officer (CISO or ISO) with adequate resources and authority
- All required security policies, procedures, and standards documented and current
- Regular security assessments demonstrate ongoing compliance
- Independent security audits validate control effectiveness
- Security requirements integrated into all technology projects and operations
- Active security governance with executive oversight
- Continuous security monitoring and improvement processes
- Example: "Our Information Security Officer (100% dedicated) leads our mature security program. We maintain a complete set of security policies aligned with SIMM 5300, reviewed annually by our Security Advisory Committee. We conduct quarterly vulnerability assessments and annual penetration testing. Our most recent independent security audit (2024) confirmed compliance with all SIMM 5300 requirements. We use a security framework (NIST CSF) mapped to state requirements. All projects undergo security review at each stage gate. We have 24/7 security monitoring and incident response capabilities. Our Security Advisory Committee (including CISO, CIO, CFO, and program representatives) meets monthly to review security metrics, incidents, and continuous improvement initiatives."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 4.2 Active Security Governance and Leadership

**Why This Matters:** Beyond baseline compliance, effective security requires active governance, leadership engagement, security-aware culture, and proactive risk management. This prevents security from being merely a checklist exercise and integrates it into organizational decision-making.

**Rating Scale and Examples:**

**Score 1 - No Security Leadership or Governance:**
- No designated Information Security Officer or security leadership role
- Security decisions made reactively in response to incidents
- No security governance structure or oversight committee
- Security is viewed as IT's responsibility, not a department-wide concern
- No security training or awareness program for staff
- Security risks are not systematically identified, assessed, or managed
- Example: "We don't have a designated security officer. Our IT Director handles security issues as they arise, usually after problems occur. We've responded to phishing attacks and malware incidents, but we don't have proactive security measures or planning. Staff receive no security training beyond an email reminder to change passwords periodically. Security is not discussed in leadership meetings or considered in strategic planning."

**Score 3 - Designated Security Officer with Limited Influence:**
- Information Security Officer designated but as additional duty (part-time, collateral assignment)
- ISO has limited staff, budget, or organizational authority
- Security committee exists on paper but meets infrequently
- Security policies exist but enforcement is inconsistent
- Basic security awareness training provided but not comprehensive
- ISO struggles to get security priorities addressed due to competing demands
- Security risks identified but remediation depends on available resources and competing priorities
- Example: "Our IT Security Specialist serves as our ISO at 50% time (remaining time on system administration duties). They've developed security policies and conduct periodic security scans. We have a Security Advisory Group that was supposed to meet quarterly but has met only twice in the past year. Our ISO provides annual security awareness training to staff. However, security improvements often take back seat to other IT priorities. When our ISO raises security concerns, remediation depends on whether budget and staff time are available after other priorities are addressed."

**Score 5 - Empowered Security Leadership with Proactive Program:**
- Dedicated Chief Information Security Officer (CISO) or ISO with full-time responsibility
- CISO has authority, budget, and staff resources appropriate to department size and risk
- Active Security Advisory Committee with executive membership and regular meetings
- Security is a standing agenda item in executive and governance meetings
- Comprehensive security awareness and training program for all staff with role-based training
- Security metrics tracked and reported to leadership regularly
- Proactive security program with threat intelligence, vulnerability management, and continuous improvement
- Security requirements integrated into all business processes, not just IT
- Strong security culture where all staff understand their security responsibilities
- Example: "Our CISO reports directly to the Deputy Director and leads a team of three security professionals (security engineer, security analyst, and compliance specialist). Our Security Advisory Committee includes our Director, Deputy Director, CIO, CFO, HR Director, and CISO; we meet monthly to review security posture, incidents, metrics, and investments. All staff complete mandatory security awareness training annually, with specialized training for IT staff, managers, and high-risk roles. We track and report key security metrics (vulnerability remediation, phishing simulation results, incident response times) to our governance board quarterly. We participate in multi-state threat intelligence sharing and conduct regular tabletop exercises. Security reviews are mandatory in our project lifecycle and procurement processes. Recent staff survey showed 91% of staff understand their security responsibilities."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

## DOMAIN 5: Service Delivery and Citizen Experience

*This domain evaluates the department's capability to deliver digital services to citizens and stakeholders, including user-centered design practices and accessibility.*

### 5.1 Digital Service Adoption and Maturity

**Why This Matters:** Modern government services should be accessible online, convenient, and efficient. Digital service maturity reduces costs, improves citizen satisfaction, increases accessibility, and enables data-driven service improvement.

**Rating Scale and Examples:**

**Score 1 - Primarily Analog, Paper-Based, and Manual Processes:**
- Most services require in-person visits or paper forms
- No online service portals or transaction capabilities
- Website provides information only; no interactive services
- Citizens must submit paper applications, visit offices, and wait for mailed responses
- Internal processing is manual (paper files, routing, manual data entry)
- No tracking or status visibility for citizens
- Example: "To apply for a permit, citizens must visit our office, complete a paper application, submit supporting documents, and pay with check or money order. Applications are manually routed through multiple reviewers using physical files. Processing takes 4-6 weeks, and applicants must call to check status. Our website provides forms for download and office hours, but no online services. All correspondence is via postal mail. Similar paper-based processes exist for licenses, complaints, and other services."

**Score 3 - Web Forms or Fillable PDFs, Backend Remains Manual:**
- Website offers downloadable or fillable PDF forms
- Some services accept online form submission (e.g., contact forms, basic applications)
- Online payment capability exists for simple transactions (fees, renewals)
- Backend processing remains largely manual (staff print submissions and manually process)
- Limited online status checking or self-service capabilities
- Hybrid model: some services online, many still require paper or in-person
- Example: "Citizens can download permit applications from our website or submit requests through an online form. We accept payments through our website for renewals and simple transactions. However, submitted forms are printed by staff and processed manually like paper applications. Citizens can check basic status (received, under review, approved) through a lookup tool, but details are limited. About 30% of our services have some online component; the remainder require in-person visits or mailed applications. Processing times have improved marginally, but most workflows remain paper-based behind the scenes."

**Score 5 - End-to-End Digital Services with Unified Portal:**
- Comprehensive online service portal where citizens can complete most transactions digitally
- Fully digital workflows: applications submitted, reviewed, approved, and fulfilled online
- Integrated account management: citizens create accounts, track all interactions, receive notifications
- Mobile-responsive design enables service access from any device
- Real-time status updates and communication (email, text, portal notifications)
- Online scheduling for services that require in-person components
- Payment integrated throughout transaction flows
- Backend systems fully digital, eliminating paper processing and manual data entry
- Services integrated across department (single portal, not separate systems for each service)
- Example: "Our unified Service Portal enables citizens to apply for permits, licenses, registrations, and submit complaints entirely online from any device. Citizens create accounts where they can manage all interactions: submit applications with document upload, track status in real-time, communicate with reviewers, receive automated notifications, pay fees, and download approved documents. 85% of our services are available online. Internal workflows are fully digital: applications route through automated workflows with electronic review, approval, and issuance. We eliminated 90% of paper processing. Average processing time decreased from 4 weeks to 5 days. Citizens can schedule inspections online. Our portal integrates with CalSAWS for eligibility verification. 73% of transactions are now completed fully online without any in-person contact."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 5.2 User-Centered Design and Accessibility

**Why This Matters:** Services should be designed based on user needs and be accessible to all Californians, including people with disabilities. User-centered design improves usability, increases adoption, and ensures equitable access. Accessibility compliance is a legal requirement and social responsibility.

**Rating Scale and Examples:**

**Score 1 - No User Research or Accessibility Considerations:**
- Systems and services designed based on internal processes and staff preferences
- No user research or testing with actual citizens or stakeholders
- Accessibility not considered in design or procurement
- Websites and applications do not meet WCAG standards
- Complaints about usability and accessibility common but not systematically addressed
- No UX design expertise in-house
- Example: "Our systems and forms are designed by IT and program staff based on our internal business processes. We don't conduct user research or usability testing with citizens. Our website and applications have not been evaluated for accessibility compliance. We occasionally receive complaints that our website is difficult to use or doesn't work with screen readers, but we haven't systematically addressed these issues. We don't have UX designers or accessibility expertise. Procurement RFPs don't include accessibility requirements."

**Score 3 - Occasional User Feedback, Basic Accessibility Efforts:**
- Some user feedback collected through surveys or informal channels
- Accessibility considered for new systems but retrofitting existing systems ongoing
- Basic accessibility testing performed, but comprehensive WCAG compliance not verified
- Some usability improvements based on feedback, but not systematic UX process
- Accessibility requirements included in some procurements
- Limited UX expertise; may use contractors for major projects
- Example: "We conduct annual user satisfaction surveys and occasionally hold focus groups when redesigning major services. We've begun addressing accessibility: our new permit portal was designed to meet WCAG 2.1 AA standards, and we're gradually remediating our older systems. We contract with an accessibility specialist to conduct periodic audits. We've made improvements based on feedback (simplified language, better navigation), but we don't have a formal UX process or in-house expertise. Accessibility is included in our IT procurement policy, but enforcement is inconsistent."

**Score 5 - Dedicated UX/User Research Team, WCAG 2.1 AA or Better:**
- User-centered design integrated into all service and system development
- In-house UX team conducts user research, usability testing, and iterative design
- All public-facing systems designed and tested with diverse users, including people with disabilities
- Comprehensive accessibility program ensures WCAG 2.1 AA or higher compliance
- Regular accessibility audits with remediation plans and tracking
- Accessibility and usability requirements mandatory in all technology procurements
- Staff trained in inclusive design and accessibility best practices
- Continuous feedback mechanisms (analytics, user testing, help desk analysis) drive ongoing improvements
- Example: "Our UX and Accessibility team (3 dedicated staff) leads user-centered design for all citizen-facing services. We conduct user research for all major projects: stakeholder interviews, usability testing sessions, accessibility testing with assistive technology users. All systems undergo accessibility reviews and WCAG 2.1 AA testing before launch. We maintain an accessibility conformance roadmap; 100% of public services meet or exceed WCAG 2.1 AA. We conduct quarterly accessibility audits and remediate issues within 30 days. Our procurement policy requires VPAT (Voluntary Product Accessibility Template) and usability standards in all RFPs, with evaluation scoring for accessibility. We use analytics, user testing, and helpdesk data to continuously improve service design. Recent user satisfaction scores: 4.2/5.0 for overall satisfaction, 4.5/5.0 for ease of use."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

## DOMAIN 6: Funding, Planning, and Portfolio Management

*This domain assesses project management maturity, portfolio prioritization discipline, and preparation of project documentation required for successful modernization.*

### 6.1 Project Management Maturity

**Why This Matters:** Strong project management practices ensure projects are delivered on time, within budget, and meeting objectives. Mature PM capabilities reduce risk, enable effective oversight, and increase the likelihood of successful outcomes.

**Rating Scale and Examples:**

**Score 1 - No Formal Methodology; Frequent Overruns:**
- No standardized project management methodology or practices
- Projects managed informally without defined processes
- No project charter, work breakdown structure, or formal project plans
- Frequent project delays, budget overruns, and scope creep
- Limited stakeholder communication and change management
- No project performance tracking or metrics
- Projects often fail to meet objectives or are abandoned
- Example: "We don't have formal project management practices. When we start a technology project, the IT Director assigns tasks to staff informally. We don't create project charters or detailed plans. Projects typically take much longer than initially expected and cost more than estimated. Our last major system implementation was supposed to take 12 months and $500K but took 28 months and cost $1.2M. Stakeholders complain about lack of communication and being surprised by changes. We don't track project metrics or conduct lessons learned."

**Score 3 - Formal PM Practices Exist, Inconsistently Applied:**
- Project management methodology exists (e.g., PMBOK, Agile) but application varies
- Some projects follow formal processes; others are managed informally
- Project managers may lack training or certification
- Project documentation exists but quality and completeness vary
- Some project tracking, but not standardized across all projects
- Project governance exists but oversight may be inconsistent
- Mixed project success rates
- Example: "We adopted the state PAL (Project Approval Lifecycle) methodology three years ago and use it for large projects requiring CDT oversight. We have project managers who have taken PM training, but not all are certified PMP or equivalent. Some projects follow structured approaches with detailed plans, regular status reporting, and stakeholder engagement. Smaller projects are still managed informally. Our project success rate has improved, but we still experience delays and budget issues on some projects. Project reporting to our governance board is inconsistent—some provide detailed dashboards, others brief verbal updates."

**Score 5 - Enterprise PMO, Standardized Methodologies, Transparent Tracking:**
- Enterprise Project Management Office (PMO) established with dedicated staff and leadership
- Standardized, documented project management methodology required for all projects
- All project managers trained and certified (PMP, CSM, or equivalent)
- Comprehensive project governance integrated with technology governance
- Standardized project documentation, templates, and tools (project management software)
- Real-time project health tracking with dashboards visible to leadership
- Regular project reviews with stage gates and go/no-go decisions
- Lessons learned captured and applied to improve practices
- Strong track record of delivering projects on time, on budget, meeting objectives
- Example: "Our Enterprise PMO (PMO Director plus 2 project managers) provides methodology, tools, training, and oversight for all department projects. We use a hybrid methodology combining waterfall (PAL) and agile practices based on project characteristics. All project managers are PMP certified and receive ongoing professional development. We use Azure DevOps for project tracking with real-time dashboards showing schedule, budget, risks, and deliverables. Projects undergo stage gate reviews at our governance board with clear criteria for advancement. We maintain a lessons learned repository and conduct retrospectives after every project. Over the past two years, we've delivered 12 major projects with 92% on-time, 95% on-budget, and 100% meeting core objectives. Our PMO maturity assessment (using OPM3 framework) scored at Level 4 (Managed)."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 6.2 Portfolio Prioritization Process

**Why This Matters:** Departments have more project needs than available resources. Effective portfolio management ensures the right projects are funded based on strategic value, risk, ROI, and organizational readiness rather than politics or who asks loudest.

**Rating Scale and Examples:**

**Score 1 - Political or Subjective Decision-Making:**
- No formal process for evaluating and prioritizing projects
- Decisions based on political considerations, urgency, or loudest advocates
- No consistent criteria for comparing projects or making trade-offs
- Resource allocation reactive rather than strategic
- No portfolio view of all projects and resource commitments
- Projects initiated without considering capacity or competing priorities
- Example: "Project decisions are made by executive leadership based on immediate needs or pressure from stakeholders. We don't have a formal prioritization process or scoring criteria. Sometimes projects are approved because of external pressure or political considerations. We don't maintain a portfolio view of all our projects and resource commitments. We frequently start new projects without considering whether we have capacity, leading to overcommitted staff and competing priorities."

**Score 3 - Some Formal Process, Not Consistently Applied:**
- Portfolio prioritization criteria have been defined (strategic alignment, ROI, risk, etc.)
- Projects supposed to be scored and ranked, but process not always followed
- Portfolio review occurs, but decisions may still be influenced by subjective factors
- Resource capacity considered but not rigorously enforced
- Portfolio management tools exist but may not be kept current
- Prioritization process may be bypassed for "urgent" or politically sensitive projects
- Example: "We have a project prioritization framework that scores projects on strategic alignment, financial benefit, risk reduction, and implementation complexity. Our governance board is supposed to review and prioritize projects quarterly. However, the process is not always followed—sometimes projects are approved outside the normal cycle due to urgency or executive directive. We have a project portfolio in a spreadsheet, but it's not always up to date. Resource capacity planning is approximate rather than detailed. The prioritization process works well for planned projects but breaks down when responding to urgent needs."

**Score 5 - Transparent, Objective Value/Risk/ROI Model:**
- Formal, documented portfolio management process with transparent criteria
- All projects evaluated using consistent scoring methodology (business value, strategic alignment, risk, cost, resource requirements)
- Portfolio review and prioritization performed regularly on published schedule
- Portfolio management tools provide real-time visibility into all projects, resources, and priorities
- Resource capacity planning ensures commitments match available capacity
- Portfolio balanced across strategic themes, risk levels, and time horizons
- Clear governance authority for portfolio decisions; process enforced consistently
- Portfolio performance metrics tracked and reported
- Example: "Our Portfolio Management Process evaluates all projects using a standardized scoring model: strategic alignment (30%), business value/ROI (25%), risk reduction (20%), implementation readiness (15%), and citizen impact (10%). Our governance board reviews the portfolio quarterly; all projects above $100K must go through this process. We use ServiceNow PPM for portfolio management, providing real-time visibility into 32 active projects, resource allocations, and pipeline. Resource capacity planning ensures we don't overcommit staff—we maintain 80% allocation to leave capacity for urgent issues. Our portfolio is balanced: 40% mission-critical, 30% service improvement, 20% infrastructure, 10% innovation. No projects are approved outside the portfolio process without formal exception approved by our Director. Portfolio health metrics (project count, budget, timeline adherence) reported monthly to executive leadership."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

### 6.3 "Stage 1-Ready" Project Documentation

**Why This Matters:** California's Project Approval Lifecycle (PAL) requires Stage 1 Business Analysis for major technology projects. Having projects with completed or nearly complete Stage 1 documentation demonstrates readiness to begin work immediately if funding is allocated.

**Rating Scale and Examples:**

**Score 1 - No Project Pipeline; No Analyses Completed:**
- No inventory of potential modernization projects
- No business case or feasibility analyses have been conducted
- No Stage 1 or pre-Stage 1 documentation exists
- Would need to start from scratch if funding became available
- Unclear what modernization priorities would be if funding were allocated
- Example: "We have general awareness that some of our systems need modernization, but we haven't documented specific project needs or conducted any formal analysis. We don't have business cases, feasibility studies, or Stage 1 documentation. If modernization funding became available, we would need to start by identifying and analyzing potential projects, which could take 6-12 months before we could begin actual implementation."

**Score 3 - Informal Concepts; No Formal Stage 1 Documentation:**
- Modernization needs identified and informally documented
- Some preliminary analysis conducted (problem statements, high-level requirements)
- Business cases may exist but not in formal Stage 1 format
- Readiness for formal Stage 1 submission is low; significant work needed
- May have vendor proposals or quotes but not comprehensive analysis
- Would require several months of work to prepare formal Stage 1 submission
- Example: "We have a list of 5-6 systems that need modernization and have documented the problems and high-level needs in white papers and presentations. We've had preliminary discussions with a couple vendors and received rough cost estimates. However, we haven't conducted the formal business analysis, alternatives analysis, or cost-benefit analysis required for Stage 1. We'd need to dedicate staff for 3-6 months to prepare proper Stage 1 documentation if we wanted to pursue funding."

**Score 5 - Multiple Projects with Completed Stage 1 Documentation:**
- Formal project pipeline with multiple modernization opportunities identified and prioritized
- Stage 1 Business Analysis completed for top-priority projects
- Comprehensive documentation including: business problem, alternatives analysis, recommended solution, cost-benefit analysis, risk assessment
- Stage 1 documents meet CDT quality standards or have already been submitted/approved
- Can immediately move to Stage 2 (procurement) when funding is allocated
- Project readiness assessment confirms organizational capacity to execute
- Example: "We have a prioritized pipeline of 8 modernization projects. We've completed full Stage 1 Business Analysis for our top 3 priorities: (1) Licensing and Permitting System replacement - Stage 1 approved by CDT in March 2025, ready for Stage 2; (2) Cloud Data Platform - Stage 1 under CDT review, submitted January 2025; (3) Inspections Mobile Application - Stage 1 draft complete, planned submission June 2025. Each Stage 1 includes comprehensive business case, alternatives analysis, recommended solution with vendor quotes, 5-year cost-benefit analysis, risk assessment, and implementation roadmap. We have approved project charters, assigned executive sponsors, and confirmed resource availability. If funding is allocated, we can immediately proceed to procurement for our first project and begin implementation within 90 days."

**Your Department's Rating:**

_Score: [ ]_

_Supporting Evidence and Comments:_
```



```

---

## ASSESSMENT SUMMARY AND SCORING

### Domain Score Calculations

Calculate the average score for each domain by averaging all indicators within that domain.

| Domain | Indicators | Total Score | Average Score |
|--------|-----------|-------------|---------------|
| **1. Leadership and Governance** | 3 indicators | ___ | ___ / 3 = ___ |
| **2. Data and Technology Infrastructure** | 3 indicators | ___ | ___ / 3 = ___ |
| **3. Organizational Culture & Workforce** | 3 indicators | ___ | ___ / 3 = ___ |
| **4. Cybersecurity & Risk Management** | 2 indicators | ___ | ___ / 2 = ___ |
| **5. Service Delivery & Citizen Experience** | 2 indicators | ___ | ___ / 2 = ___ |
| **6. Funding, Planning & Portfolio Management** | 3 indicators | ___ | ___ / 3 = ___ |

### Overall Readiness Score Calculation

Apply the recommended domain weights to calculate your overall readiness score:

| Domain | Domain Score | Weight | Weighted Score |
|--------|--------------|--------|----------------|
| Leadership and Governance | ___ | 20% | ___ × 0.20 = ___ |
| Data and Technology Infrastructure | ___ | 20% | ___ × 0.20 = ___ |
| Organizational Culture & Workforce | ___ | 15% | ___ × 0.15 = ___ |
| Cybersecurity & Risk Management | ___ | 15% | ___ × 0.15 = ___ |
| Service Delivery & Citizen Experience | ___ | 15% | ___ × 0.15 = ___ |
| Funding, Planning & Portfolio Management | ___ | 15% | ___ × 0.15 = ___ |
| **TOTAL OVERALL READINESS SCORE** | | **100%** | **___** |

### Readiness Tier Classification

Based on your overall readiness score:

- **Tier 1 (Ready Now)**: Score 4.0 - 5.0 → Immediate funding priority
- **Tier 2 (Near-Ready)**: Score 3.0 - 3.9 → Funding with targeted support
- **Tier 3 (Developing)**: Score 2.0 - 2.9 → Capability development focus
- **Tier 4 (Early Stage)**: Score 1.0 - 1.9 → Foundational capacity building

**Your Department's Tier: ___________**

---

## QUALITATIVE ASSESSMENT

### Key Strengths

Identify 3-5 areas where your department demonstrates the strongest capabilities:

1. 
2. 
3. 
4. 
5. 

### Key Gaps and Challenges

Identify 3-5 areas representing the most significant readiness gaps or challenges:

1. 
2. 
3. 
4. 
5. 

### Priority Improvement Opportunities

Based on your assessment, what are the top 3 areas where targeted investment or focus would most improve your readiness?

1. 
2. 
3. 

### Readiness for Specific Projects

If you have identified specific modernization projects, assess your readiness for each:

**Project 1:** _______________
- Readiness Level (High/Medium/Low): ___
- Specific gaps or prerequisites: ___

**Project 2:** _______________
- Readiness Level (High/Medium/Low): ___
- Specific gaps or prerequisites: ___

**Project 3:** _______________
- Readiness Level (High/Medium/Low): ___
- Specific gaps or prerequisites: ___

---

## SUPPORTING DOCUMENTATION

Attach or reference supporting documentation that validates your assessment:

- [ ] IT Strategic Plan
- [ ] Technology Governance Charter
- [ ] Organizational Chart (IT and technology-related roles)
- [ ] Cloud Adoption Strategy or Roadmap
- [ ] Data Governance Policies or Framework
- [ ] Security Policies and SIMM 5300 Compliance Documentation
- [ ] Recent Security Audit or Assessment Results
- [ ] Project Portfolio List
- [ ] Stage 1 Business Analysis Documents (if available)
- [ ] Project Management Methodology Documentation
- [ ] Recent Project Performance Reports or Dashboards
- [ ] User Satisfaction Survey Results
- [ ] Accessibility Audit or VPAT Documentation
- [ ] Other: _______________

---

## EXECUTIVE CERTIFICATION

I certify that this assessment accurately represents the current state of our department's technology readiness. The assessment was completed collaboratively by appropriate subject matter experts, and the ratings and supporting evidence are truthful and complete to the best of our knowledge.

**Department:** _______________________________________________

**Assessment Completion Date:** _______________

**Primary Contact for Questions:**
- Name and Title: _______________________________________________
- Email: _______________________________________________
- Phone: _______________________________________________

**Executive Sponsor Approval:**
- Name and Title: _______________________________________________
- Signature: ___________________________________ Date: _______________

---

## SUBMISSION INSTRUCTIONS

1. **Complete all sections** of this assessment form
2. **Gather supporting documentation** listed above
3. **Obtain executive approval** and signature
4. **Submit assessment package** to: [CDT Contact/Portal Information]
5. **Submission deadline**: _______________

For questions or technical assistance during the assessment process, contact:
- Email: technology.readiness@cdt.ca.gov
- Office Hours: [Schedule and contact information]
- Resource Portal: [URL]

---

**Thank you for completing the California Technology Readiness Assessment. Your honest and thorough assessment will help ensure that modernization funding is allocated strategically to achieve the greatest impact for California residents.**