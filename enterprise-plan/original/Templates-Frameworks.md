# California State Government Modernization Strategy
## Supporting Frameworks & Templates

---

## 1. Governance Charter Template

**Purpose:** This template provides the standard structure for all governance bodies in the E3 Operational Model (Statewide E3 Council, Executive Technology Council, Technical Architecture Review Board, Data Governance Councils).

---

### **[Governance Body Name] Charter**

**Version:** 1.0  
**Effective Date:** [Date]  
**Review Cycle:** Annual

---

### **1.1 Purpose and Scope**

**Purpose Statement:**
[Describe why this governance body exists and what problem it solves]

**Example (Statewide E3 Council):**
The Statewide E3 Council exists to coordinate modernization efforts across all California agencies, ensure alignment with statewide priorities, share best practices, resolve cross-agency dependencies, and provide unified strategic direction for the Enterprise Efficiency and Effectiveness operational model.

**Scope:**
[Define what is in scope and out of scope for this body]

**In Scope:**
- [List specific responsibilities]
- [Decision authorities]
- [Areas of focus]

**Out of Scope:**
- [List what this body does NOT govern]
- [Responsibilities of other bodies]

---

### **1.2 Roles and Responsibilities**

**Chair:**
- **Position:** [Title of chair, e.g., "Statewide Chief Data Officer"]
- **Appointed by:** [Authority, e.g., "Secretary of Government Operations Agency"]
- **Term:** [Duration, e.g., "Serves at pleasure of appointing authority"]
- **Responsibilities:**
  - Set meeting agendas and facilitate discussions
  - Ensure decisions are documented and communicated
  - Represent governance body to executive leadership
  - Break tie votes when necessary
  - Escalate unresolved issues to [escalation authority]

**Members:**
- **Composition:** [List member positions or selection criteria]
  - Example: All Agency Undersecretaries for E3
  - Example: One representative from each major department
- **Appointment Process:** [How members are selected]
- **Term:** [Duration of membership]
- **Responsibilities:**
  - Attend meetings and participate actively in discussions
  - Represent their organization's perspectives
  - Implement decisions within their areas of responsibility
  - Provide status updates on assigned initiatives
  - Escalate barriers requiring governance intervention

**Ex-Officio Members (Non-Voting):**
- [Positions that attend but don't vote, e.g., "TMO Director, State CTO"]
- **Role:** Provide expertise, context, and support without voting authority

**Staff Support:**
- **Position:** [Who provides administrative support, e.g., "TMO Governance Coordinator"]
- **Responsibilities:**
  - Schedule meetings and manage logistics
  - Prepare agendas and pre-read materials
  - Document decisions and action items
  - Track action item completion
  - Maintain governance records and archives

---

### **1.3 Decision-Making Authorities**

**Approval Authority:**

[List specific decisions this body can approve, with any dollar thresholds or conditions]

**Examples:**
- Approve technology investments between $5M-$25M (Executive Technology Council approves >$25M)
- Mandate use of enterprise standards and shared services
- Approve or reject data sharing agreements between agencies
- Prioritize cross-agency initiatives and allocate resources

**Recommendation Authority:**

[List decisions this body can recommend but requires higher approval]

**Examples:**
- Recommend budget allocations to Governor's Office
- Recommend policy changes to Legislature
- Recommend organizational structure changes to Agency Secretaries

**No Authority:**

[List decisions explicitly outside this body's purview]

**Examples:**
- Cannot override individual agency personnel decisions
- Cannot reallocate agency base budgets
- Cannot mandate changes to collective bargaining agreements

**Decision-Making Process:**

**Consensus Preferred:**
Seek consensus on all decisions. Consensus means all members can support the decision, even if not their first preference.

**Voting When Necessary:**
- **Quorum:** [Percentage needed for valid vote, e.g., "60% of voting members present"]
- **Threshold:** [Percentage needed to pass, e.g., "Simple majority of those present" or "Two-thirds supermajority"]
- **Tie-Breaking:** Chair breaks ties
- **Abstentions:** Members may abstain if conflict of interest; abstentions don't count toward majority calculation

**Escalation Path:**
If governance body cannot reach decision after reasonable deliberation:
1. Document the issue, options considered, and areas of disagreement
2. Escalate to [next higher authority]
3. Timeline for escalation response: [e.g., "Within 2 weeks"]

---

### **1.4 Meeting Cadence and Structure**

**Regular Meetings:**
- **Frequency:** [Monthly, Quarterly, etc.]
- **Duration:** [Typical meeting length, e.g., "2 hours"]
- **Schedule:** [Day/time, e.g., "Third Thursday of each month, 10am-12pm"]
- **Location:** [Hybrid: Virtual with optional in-person at specified location]

**Special Meetings:**
- **Called by:** Chair or by request of [X number/percentage] of members
- **Notice:** [Advance notice required, e.g., "Minimum 5 business days"]
- **Purpose:** Address urgent issues that cannot wait for regular meeting

**Meeting Structure:**

**Pre-Meeting (1 Week Before):**
- Agenda and materials distributed to all members
- Members submit agenda additions or questions
- Staff support prepares decision papers and analysis

**Meeting Flow:**
1. **Opening (10 minutes):** Review agenda, approve prior meeting minutes, confirm decision items
2. **Information Items (30 minutes):** Status updates, presentations, environmental scan
3. **Discussion Items (45 minutes):** Policy development, problem-solving, strategic planning
4. **Decision Items (30 minutes):** Vote on proposals requiring approval
5. **Closing (5 minutes):** Review action items, confirm next meeting, adjourn

**Post-Meeting (Within 3 Days):**
- Minutes and decisions published to governance portal
- Action items assigned with deadlines
- Communications to stakeholders about decisions

**Materials Requirements:**
- **Decision Papers:** Submitted 1 week before meeting, include problem statement, options analysis, recommendation, financial impact, implementation plan
- **Presentations:** Max 15 minutes, leave time for discussion
- **Pre-Read:** Materials should be self-explanatory; meeting time for discussion not presentation

---

### **1.5 Performance Accountability**

**Governance Body Performance Metrics:**

**Meeting Effectiveness:**
- **Target:** 90%+ member attendance at regular meetings
- **Target:** 80%+ decisions made on first presentation (not tabled for additional analysis)
- **Target:** 100% action items completed on time or formally extended

**Decision Quality:**
- **Measure:** Percentage of decisions requiring reversal or significant revision
- **Target:** <10% reversal rate

**Impact:**
- **Measure:** Progress on strategic objectives under governance body purview
- **Target:** Achieve 70%+ of annual OKRs

**Member Accountability:**

**Attendance:**
- Members missing >25% of meetings without valid reason may be replaced
- Send designated alternate if unable to attend (alternate has full voting authority for that meeting)

**Participation:**
- Members expected to review materials before meetings and come prepared
- Members complete assigned action items by deadlines
- Members represent governance body decisions within their organizations (even if personally disagreed)

**Annual Self-Assessment:**
- Governance body conducts annual effectiveness review
- Survey members on meeting quality, decision-making process, impact
- Identify improvements for following year
- Results shared with [oversight authority]

---

### **1.6 Escalation Pathways**

**When to Escalate:**
- Governance body cannot reach consensus after reasonable deliberation
- Decision required exceeds governance body authority
- Implementation barriers require higher-level intervention
- Significant risks or issues need executive awareness

**Escalation Process:**

**Level 1: Chair Mediation**
- Chair works with dissenting members to find compromise
- Timeline: Resolved within one meeting cycle

**Level 2: Escalation to Higher Governance Body**
- Document issue, options, and recommendation
- Present to [next higher authority, e.g., "Executive Technology Council"]
- Timeline: Decision within 2 weeks

**Level 3: Executive Intervention**
- Issues escalated to [e.g., "Governor's Office of Operations"]
- Reserved for strategic/political decisions
- Timeline: As rapidly as needed based on urgency

**Emergency Escalation:**
- For urgent issues (security incidents, system failures, legal compliance)
- Chair authorized to escalate immediately to appropriate executive
- Governance body notified within 24 hours

---

### **1.7 Amendment Process**

**Charter Amendments:**
- Proposed by any member or by Chair
- Requires [supermajority, e.g., "Two-thirds vote"] of governance body
- Submitted to [approval authority] for final approval
- Amendments effective upon approval
- Version control maintained; history of amendments documented

**Annual Review:**
- Charter reviewed annually for continued relevance
- Recommend updates based on lessons learned
- Alignment with evolving strategic priorities

---

## 2. KPI Dashboard Framework

**Purpose:** Standard structure for performance measurement across the modernization initiative, ensuring consistent metrics and reporting.

---

### **2.1 Dashboard Structure**

**Four-Tier Hierarchy:**

**Tier 1: Statewide Strategic KPIs**
- **Audience:** Governor, Legislature, Public
- **Update Frequency:** Quarterly
- **Visualization:** High-level trends and progress toward targets

**Tier 2: Operational KPIs**
- **Audience:** IT and Program Managers
- **Update Frequency:** Monthly
- **Visualization:** Detailed performance metrics by system/service

**Tier 3: Transformation Progress KPIs**
- **Audience:** E3 Leaders, TMO
- **Update Frequency:** Monthly
- **Visualization:** Program health indicators

**Tier 4: Organizational Change KPIs**
- **Audience:** HR, Change Management, E3 Leaders
- **Update Frequency:** Quarterly
- **Visualization:** Cultural indicators and adoption metrics

---

### **2.2 KPI Template**

**For Each Metric:**

**Metric Name:** [Short, descriptive title]

**Definition:** [Precise definition of what is being measured]

**Rationale:** [Why this metric matters and what it tells us]

**Calculation Method:**
- **Formula:** [Mathematical formula or data source]
- **Example:** Digital Service Adoption = (# Transactions Available Digitally / Total # Transactions) × 100%

**Data Sources:**
- **Primary:** [Where data comes from]
- **Frequency:** [How often data is collected]
- **Owner:** [Who is responsible for data accuracy]

**Targets:**
- **Baseline:** [Current or starting value]
- **Year 1 Target:** [Realistic improvement]
- **Year 3 Target:** [Stretch goal]
- **Year 5 Target:** [Aspirational but achievable]

**Measurement Frequency:**
- **Data Collection:** [How often measured]
- **Reporting:** [How often published to dashboards]

**Visualization Guidance:**
- **Chart Type:** [Line graph, bar chart, gauge, etc.]
- **Color Coding:**
  - Green: Meeting or exceeding target
  - Yellow: Within 10% of target (at risk)
  - Red: More than 10% below target (intervention needed)

**Accountability:**
- **Owner:** [Position responsible for achieving target]
- **Escalation:** [Who to notify if metric is Red for 2+ consecutive periods]

**Example KPI - Digital Service Adoption Rate:**

**Metric Name:** Digital Service Adoption Rate

**Definition:** Percentage of citizen-facing government transactions that can be completed entirely online without in-person visit or phone call

**Rationale:** Measures progress toward digital-first government. Higher adoption reduces cost, increases citizen convenience, and enables 24/7 service access.

**Calculation Method:**
- **Formula:** (# Services Fully Digital / Total # Citizen-Facing Services) × 100%
- **Service Inventory:** Maintained by TMO, validated by agencies quarterly
- **Fully Digital Definition:** Citizen can complete entire transaction online from any device without human assistance

**Data Sources:**
- **Primary:** Service inventory database (TMO maintained)
- **Validation:** Agency quarterly certification of service status
- **Frequency:** Quarterly census of all services

**Targets:**
- **Baseline (Year 0):** 38% (estimated; confirm with actual inventory)
- **Year 1 Target:** 45%
- **Year 2 Target:** 55%
- **Year 3 Target:** 65%
- **Year 5 Target:** 75%

**Measurement Frequency:**
- **Data Collection:** Quarterly service inventory update
- **Reporting:** Published to public dashboard within 2 weeks of quarter end

**Visualization Guidance:**
- **Chart Type:** Line graph showing quarterly progress with target line
- **Drill-Down:** Bar chart by agency showing each agency's adoption rate
- **Color Coding:**
  - Green: ≥ quarterly target
  - Yellow: Within 5 percentage points of target
  - Red: >5 percentage points below target

**Accountability:**
- **Owner:** Each Agency Undersecretary for E3 (for their agency)
- **Statewide Aggregation:** TMO Director
- **Escalation:** If statewide rate is Red for 2 consecutive quarters, escalate to Executive Technology Council

---

### **2.3 Dashboard Visualization Standards**

**Public-Facing Dashboard (Citizen Audience):**

**Design Principles:**
- Simple, clean visual design (no clutter)
- Accessible (WCAG 2.1 AA compliant, screen reader friendly)
- Mobile-responsive (50%+ of traffic will be mobile)
- Plain language (no jargon or acronyms without explanation)
- Fast loading (<3 seconds on average connection)

**Required Components:**
1. **Overview Section:** Headline metrics (digital adoption %, CSAT score, cost savings)
2. **Progress Tracker:** Visual showing completion toward strategic goals
3. **Recent Wins:** Highlight 3-5 recent improvements citizens will notice
4. **Agency Scorecards:** Drill-down into each agency's performance
5. **About/Methodology:** Explain what's being measured and why

**Update Schedule:** Weekly data refresh (automated from source systems)

**Internal Dashboard (E3 Leaders/TMO):**

**Additional Components:**
- Budget and financial tracking (spending vs. plan)
- Resource allocation (staff, contractors)
- Risk register with status
- Dependency tracking between projects
- Detailed technical metrics (API performance, system uptime, security incidents)
- Project portfolio health (on track, at risk, delayed by count and $)

**Update Schedule:** Daily or real-time where possible

**Technology Platform:**
- **Options:** Tableau, PowerBI, Domo, or open-source (Grafana, Apache Superset)
- **Requirements:** Cloud-hosted, API integration with data sources, role-based access control, export capabilities

---

### **2.4 Reporting Cadence**

**Weekly:**
- **Automated Alerts:** Email to E3 leaders flagging metrics in Red status
- **TMO Internal Review:** Review all metrics, identify trends, prepare for weekly leadership briefing

**Monthly:**
- **E3 Council Dashboard Review:** Standing agenda item at monthly E3 Council meetings
- **Detailed Reports:** By agency and by major initiative, circulated to stakeholders

**Quarterly:**
- **Public Dashboard Update:** Published to Envision 2026 portal or modernization website
- **Executive Report:** Narrative report to Governor, Agency Secretaries, Legislature (see template below)
- **Statewide Town Hall:** TMO Director presents results in public webinar, Q&A

**Annually:**
- **Comprehensive Performance Report:** Detailed analysis of full year, trends, successes, challenges
- **Benchmarking:** Compare California performance to other states and best practices
- **Strategic Review:** Assess whether KPIs still align with goals; recommend changes

---

### **2.5 Quarterly Executive Report Template**

**California State Government Modernization**  
**Quarterly Performance Report - Q[X] [Year]**

**Prepared by:** Transformation Management Office  
**Date:** [Date]  
**Distribution:** Governor, Agency Secretaries, Legislative Leadership

---

**Executive Summary (1 Page)**

**Headline Results:**
- [3-5 key accomplishments this quarter]
- [Major metrics: digital adoption %, CSAT, legacy reduction %, cost savings]

**Challenges:**
- [1-3 significant obstacles encountered]
- [Mitigation actions taken]

**Decisions Needed:**
- [List any decisions requiring executive intervention]

**Next Quarter Priorities:**
- [3-5 key focus areas]

---

**Progress Against Strategic KPIs (2-3 Pages)**

**For Each Tier 1 KPI:**
- Current value vs. target
- Trend (improving, stable, declining)
- Analysis: What's driving performance?
- Actions taken this quarter
- Outlook for next quarter

[Include charts showing trends over time]

---

**Financial Summary (1 Page)**

**Budget Performance:**
- Approved budget: $[X]M
- Spent to date: $[Y]M ([Z]% of budget)
- Projected year-end spending: $[A]M
- Variance explanation

**Return on Investment:**
- Cost savings achieved to date: $[B]M
- Cost avoidance: $[C]M
- Projected 5-year ROI: [X]%

---

**Project Portfolio Health (1-2 Pages)**

**Portfolio Overview:**
- Total active projects: [N]
- Total investment: $[X]M
- Status breakdown:
  - On Track: [N] projects ($[X]M)
  - At Risk: [N] projects ($[Y]M)
  - Delayed: [N] projects ($[Z]M)

**Completions This Quarter:**
- [List 3-5 major projects completed]
- [Impact of each]

**Projects At Risk:**
- [List projects in Red status]
- [Brief description of issues and mitigation plans]

---

**Risk Assessment (1 Page)**

**Top 5 Risks:**

For each:
- **Risk Description:** [What could go wrong]
- **Impact:** [High/Medium/Low]
- **Probability:** [High/Medium/Low]
- **Mitigation:** [What we're doing about it]
- **Owner:** [Who is responsible]

---

**Success Stories (1 Page)**

**Highlight 2-3 transformations that demonstrate tangible citizen or operational impact:**

**Example:**
**Title:** DMV Appointment Scheduling Goes Digital

**Challenge:** 2 million+ citizens wait in line annually for DMV appointments; average wait time 90 minutes

**Solution:** Deployed online appointment scheduling system; integrated with driver's license renewal and Real ID processing

**Results:**
- 65% of appointments now scheduled online
- Average wait time reduced to 22 minutes
- Citizen satisfaction increased from 58% to 82%
- Cost per appointment reduced from $18 to $7

**Quote:** "I was able to schedule my Real ID appointment on my phone during lunch and was in and out in 15 minutes. Huge improvement!" - Maria S., Los Angeles

---

**Upcoming Quarter Milestones (1 Page)**

**Major Deliverables:**
- [List 5-10 significant milestones expected next quarter]
- [Include go-lives, pilot completions, major procurements]

**Governance Activities:**
- Executive Technology Council meetings and decisions
- E3 Council priorities
- Innovation Fellowship cohort rotations

**Risks to Watch:**
- [3-5 potential challenges on the horizon]

---

**Appendices**
- Detailed KPI tables (all metrics)
- Project portfolio complete list
- Acronym glossary

---

## 3. RFI² Implementation Guide

**Purpose:** Step-by-step guide for agencies issuing Request for Innovative Ideas (RFI²) solicitations, with templates and checklists.

---

### **3.1 When to Use RFI²**

**Ideal Use Cases:**
✓ Complex technical challenges without obvious solutions  
✓ Emerging technology applications (AI, blockchain, quantum, etc.)  
✓ Cross-cutting problems affecting multiple agencies  
✓ Market innovation needed (better results than current capabilities)  
✓ High uncertainty requiring experimentation before full commitment

**Not Appropriate For:**
✗ Commodity purchases (standard hardware/software)  
✗ Well-defined requirements with known solutions  
✗ Urgent procurements (RFI² takes 9-12 months total)  
✗ Low-dollar, low-complexity needs  
✗ Situations where agency has already decided on approach

**Budget Allocation:**
Reserve 15-25% of technology modernization budget for RFI² projects focusing on highest-impact, highest-uncertainty challenges.

---

### **3.2 Phase 1: Innovation Concept Papers**

**Step 1: Problem Statement Development (Weeks 1-4)**

**Internal Workshop:**
- Assemble team: Agency subject matter experts, frontline staff who experience problem, technical leads, California Breakthrough Project advisor (if applicable)
- Duration: 2-3 half-day sessions
- Outputs: Problem definition, desired outcomes, constraints, evaluation criteria

**Problem Statement Template:**

---

**[Agency Name] Request for Innovative Ideas**  
**RFI² Number:** [Unique ID]  
**Publication Date:** [Date]

**Problem Title:** [Concise description, e.g., "Predictive Maintenance for State Infrastructure"]

**1. Background and Context**

[2-3 paragraphs explaining the problem, why it matters, and current state]

Example:
"California maintains 50,000+ miles of highways, 13,000 bridges, and thousands of state buildings. Current maintenance approaches are reactive—we fix things when they break. This leads to emergency repairs costing 3-5× planned maintenance, service disruptions affecting millions of citizens, and safety risks. Annual maintenance budget exceeds $2B, yet deferred maintenance backlog grows yearly. We seek innovative approaches to predicting when infrastructure will fail, enabling proactive maintenance, reducing costs, and improving safety."

**2. Desired Outcomes**

[Specific, measurable outcomes—what would success look like?]

Example:
- Reduce emergency repairs by 50% through predictive identification of failures
- Decrease maintenance cost per mile/structure by 30%
- Improve infrastructure availability (reduce closures/downtime) by 40%
- Extend average infrastructure lifespan by 20%

**3. Constraints and Requirements**

[Parameters vendors must work within]

Example:
- **Budget Range:** Proof of Concept $50K-$100K per vendor; full implementation $5M-$10M
- **Timeline:** 90-day PoC, 18-month implementation
- **Compliance:** All data must remain in California or approved jurisdictions; NIST security standards; accessibility (WCAG 2.1 AA)
- **Integration:** Must integrate with existing asset management systems (specify platforms)

**4. Evaluation Criteria**

[How concepts will be assessed—be transparent]

Example:
- **Innovation Potential (40%):** How novel is the approach? Does it represent significant improvement over current state?
- **Feasibility (30%):** Is it technically viable? Can it be implemented at California's scale? Any proof of concept or pilots elsewhere?
- **Cost-Effectiveness (20%):** What's the ROI? Total cost of ownership?
- **Vendor Qualifications (10%):** Does team have relevant experience and capability?

**5. Concept Paper Requirements**

[What vendors must submit]

- **Executive Summary:** 1 page high-level overview
- **Technical Approach:** 3-5 pages describing solution (how it works, technologies used, why it's innovative)
- **Feasibility Evidence:** Prior implementations, research, prototypes, pilot results
- **Team Qualifications:** Relevant experience, key personnel bios, partnerships
- **Cost Estimate:** ROM for PoC and full implementation
- **Implementation Timeline:** High-level milestones
- **Length:** 10-15 pages (excluding team bios and appendices)

**6. Submission Instructions**

- **Deadline:** [Date and time]
- **Format:** PDF submitted via [procurement portal or email]
- **Questions:** Submit questions to [contact] by [date]; answers published publicly
- **Selection Timeline:** Evaluation complete by [date], vendors notified by [date]
- **Phase 2 Details:** Selected vendors (anticipated 2-5) will receive $[X] contracts to develop Proof of Concept over 90 days

**7. Additional Information**

- Link to Innovation Conference details (see Step 2)
- Contact information for questions
- Legal terms and conditions
- Non-binding nature of concept papers (state not obligated to proceed)

---

**Step 2: Innovation Conference (Week 5)**

**Purpose:** Educate market, generate excitement, enable collaboration, clarify expectations

**Planning Checklist:**

Pre-Conference (4 weeks before):
- [ ] Reserve venue (capacity 100-200) or plan virtual event
- [ ] Announce conference date/details in RFI² publication
- [ ] Invite vendors (procurement portal, industry newsletters, targeted outreach)
- [ ] Prepare presentations (problem deep-dive, Q&A with agency experts)
- [ ] Invite California Breakthrough Project advisors as speakers
- [ ] Create breakout session structure

Conference Day:
- [ ] Registration and networking (30 min)
- [ ] Welcome and overview (15 min) - Agency leadership explains strategic importance
- [ ] Problem statement presentation (45 min) - Deep dive by subject matter experts
- [ ] Q&A with government stakeholders (45 min)
- [ ] Break and networking (30 min)
- [ ] Breakout sessions by solution approach (60 min) - Vendors discuss ideas, form teams
- [ ] Closing remarks (15 min) - Recap timeline, expectations, Q&A

Post-Conference:
- [ ] Publish presentation materials and FAQ online
- [ ] Share attendee list (with permission) enabling networking
- [ ] Send follow-up email with submission reminders

**Step 3: Concept Paper Evaluation (Weeks 16-20)**

**Evaluation Panel Composition:**

- **Agency SMEs (40%):** 3-4 people who understand problem deeply
- **Technical Experts (30%):** 2-3 architects/engineers from CDT/ODI
- **External Advisors (20%):** 1-2 California Breakthrough Project advisors or academic researchers
- **Procurement/Legal (10%):** 1 person ensuring process compliance

**Total Panel Size:** 7-10 people

**Evaluation Process:**

**Week 1: Individual Scoring**
- Each panelist independently reviews all concept papers
- Score using standardized rubric (0-5 scale for each criterion)
- Document strengths, weaknesses, questions for each proposal
- No discussion with other panelists

**Week 2: Consensus Meeting**
- Panel convenes to discuss all proposals
- Identify top-scoring proposals (typically top 30-40%)
- Discuss disagreements and reach consensus on rankings
- May invite top vendors for clarification presentations (30 min each)

**Week 3: Final Selection**
- Panel recommends 2-5 vendors for Phase 2
- Considerations: Diversity of approaches (don't select 5 identical ideas), vendor capability, budget constraints
- Document rationale for selections and non-selections

**Week 4: Approvals and Notifications**
- Obtain agency leadership approval of selections
- Notify selected vendors (by phone, then formal letter)
- Notify non-selected vendors (thank them for participation, offer debrief)

**Step 4: Phase 1 Contract Awards (Week 21)**

**Contract Details:**
- **Award Amount:** $25K-$50K per vendor (compensates for PoC development effort)
- **Deliverable:** Working Proof of Concept demonstration
- **Timeline:** 90 days from contract execution
- **Terms:** Standard state contract terms, IP provisions

---

### **3.3 Phase 2: Proof of Concept Development**

**Step 5: PoC Planning (Weeks 22-26)**

**Kickoff Meetings with Each Vendor:**

Agenda:
- Refine scope based on concept paper feedback
- Define specific success criteria (quantitative metrics)
- Identify data needed from agency (plan for sanitization/anonymization)
- Determine environment (vendor cloud, state cloud, hybrid)
- Establish check-in cadence (biweekly recommended)
- Clarify evaluation process and timeline

**PoC Plan Template:**

---

**Proof of Concept Plan**

**Vendor:** [Name]  
**Agency:** [Name]  
**RFI² Title:** [Problem statement]  
**PoC Timeline:** [Start date] - [End date]

**1. Scope**

[What specific capabilities will be demonstrated in PoC]

Example: "Demonstrate predictive model for bridge maintenance by analyzing 5 years of inspection data for 100 sample bridges, predicting which bridges will require maintenance in next 12 months, and comparing predictions to actual maintenance needs."

**2. Success Criteria**

[Quantitative metrics defining success]

Example:
- Prediction accuracy ≥75% (correctly identify bridges needing maintenance)
- False positive rate ≤20% (don't over-predict)
- Demonstration of cost savings (estimate $ saved by proactive vs. reactive maintenance)
- Usability: Agency staff can operate system with <4 hours training

**3. Data Requirements**

[What data is needed from agency]

Example:
- 5 years bridge inspection reports (PDF and structured data if available)
- Maintenance logs (work orders, costs, dates)
- Bridge inventory (age, materials, traffic volume, location)
- **Agency Responsibilities:** Sanitize data (remove PII, sensitive locations), provide by [date]
- **Vendor Responsibilities:** Secure data handling (encryption, access controls), return or destroy data at PoC end

**4. Environment**

[Where PoC will run]

Example:
- **Development:** Vendor cloud environment (AWS/Azure/GCP)
- **Demonstration:** Hybrid (vendor hosts backend, agency accesses via web interface)
- **Security:** Vendor follows [NIST 800-53 controls]; provides security assessment documentation

**5. Milestones**

| Week | Milestone | Deliverable |
|------|-----------|-------------|
| 2 | Data ingestion complete | Confirmation data loaded and accessible |
| 4 | Initial model trained | Preliminary results on sample data |
| 6 | Full model development | Model predicting on all 100 bridges |
| 8 | User interface complete | Functional demo environment |
| 10 | Internal testing | Vendor QA complete, ready for agency testing |
| 12 | Agency user testing | Feedback incorporated |
| 13 | Final demonstration | Presentation to evaluation panel |

**6. Check-In Schedule**

- **Frequency:** Biweekly (every 2 weeks)
- **Format:** 1-hour video conference
- **Attendees:** Vendor lead, agency SME, technical contact
- **Agenda:** Progress update, demo of current state, roadblocks discussion, next steps

**7. Evaluation Process**

[How PoC will be assessed]

- **Demonstration Date:** [Date]
- **Format:** 2-hour session (1 hour presentation, 1 hour hands-on testing by evaluation panel)
- **Evaluation Panel:** [Same panel as Phase 1 or refreshed composition]
- **Scoring:** Using criteria defined in RFI² (technical performance, usability, cost-effectiveness, scalability, vendor capability)
- **Timeline:** Decision within 2 weeks of final demonstration

---

**Step 6: PoC Development (Weeks 27-39)**

**Agency Support During PoC:**

**Biweekly Check-Ins:**
- Review progress against milestones
- Provide feedback on interim demonstrations
- Address vendor questions and roadblocks
- Document learnings

**Data and Access:**
- Provide data as scheduled
- Grant access to systems/environments if needed
- Respond to technical questions within 2 business days

**Adjustments:**
- If vendor is struggling, provide support or adjust scope (goal is learning, not vendor failure)
- If vendor is ahead of schedule, consider expanding scope
- Document any changes to PoC plan

**Step 7: PoC Evaluation (Weeks 40-44)**

**Demonstration Format:**

**Part 1: Presentation (60 minutes)**
- Vendor presents approach, results, lessons learned
- Live demonstration of working prototype
- Q&A with evaluation panel

**Part 2: Hands-On Testing (60 minutes)**
- Evaluation panel members test system with realistic scenarios
- Agency staff (end users) try using the system
- Assess usability, performance, functionality

**Evaluation Scoring:**

**Scorecard Template:**

| Criterion | Weight | Score (0-5) | Weighted Score | Comments |
|-----------|--------|-------------|----------------|-----------|
| **Technical Performance** | 40% | | | Does it work? How well vs. requirements? |
| - Accuracy/Effectiveness | 20% | | | |
| - Performance/Speed | 10% | | | |
| - Scalability | 10% | | | |
| **Usability** | 20% | | | Can agency staff use it? |
| - User Interface Quality | 10% | | | |
| - Training Required | 5% | | | |
| - Documentation | 5% | | | |
| **Cost-Effectiveness** | 15% | | | ROI vs. alternatives? |
| - PoC Budget Performance | 5% | | | |
| - Projected Implementation Cost | 5% | | | |
| - Operating Cost Estimate | 5% | | | |
| **Scalability & Maintainability** | 15% | | | Can this grow? Can we support it? |
| - Technical Architecture | 8% | | | |
| - Vendor Support Model | 7% | | | |
| **Vendor Capability** | 10% | | | Can they deliver at scale? |
| - Team Expertise | 5% | | | |
| - Implementation Approach | 5% | | | |
| **TOTAL** | 100% | | | |

**Scoring Scale:**
- 5 = Exceptional (exceeds expectations significantly)
- 4 = Very Good (exceeds expectations moderately)
- 3 = Good (meets expectations)
- 2 = Fair (partially meets expectations)
- 1 = Poor (does not meet expectations)
- 0 = Unacceptable (complete failure)

**Consensus Process:**
- Individual scoring first (no discussion)
- Panel meeting to discuss scores and reach consensus
- Document rationale for final scores

**Step 8: Implementation Contract Award (Week 45+)**

**Award Decision Options:**

**Option 1: Single Award**
- One vendor clearly superior
- Award full implementation contract
- Typical contract: $5M-$10M over 18-24 months

**Option 2: Multiple Awards**
- Different vendors excel at different aspects
- Award contracts for different components (e.g., one vendor for prediction model, another for user interface)
- Award pilot deployments in different contexts (e.g., bridges vs. highways)

**Option 3: Phased Award**
- Award limited deployment contract to top vendor
- Expand based on results
- Reduces risk of full commitment before proven at scale

**Option 4: No Award**
- None of the PoCs meet requirements
- Return to market with revised approach, or
- Abandon initiative (valuable learning even if no solution found)

**Implementation Contract Structure:**

**Base Period (12-18 months):**
- Full development and deployment
- Training and documentation
- Initial operations support

**Option Years (3-5 years):**
- Ongoing operations and maintenance
- Enhancement and feature development
- User support

**Performance Metrics:**
- Payment tied to achieving outcomes (not just effort)
- Example: 50% payment on go-live, 25% on achieving 75% user adoption, 25% on achieving ROI targets

**Transition Plan:**
- If contract ends, vendor must support transition to different solution
- Data extraction and handover requirements
- Knowledge transfer to state staff

---

### **3.4 RFI² Success Metrics**

**Track these metrics for each RFI² to assess process effectiveness:**

**Participation:**
- Number of concept papers received (target: 15-30+)
- Diversity of vendors (large/small, established/startup, in-state/national)
- Innovation Conference attendance

**Quality:**
- Evaluation panel assessment of concept paper quality
- Number of viable PoC vendors (target: 2-5)
- PoC success rate (achieve success criteria)

**Outcomes:**
- Implementation contract awarded? (success if yes, learning if no)
- Delivered outcomes vs. original problem statement
- ROI achieved (cost savings, service improvements)

**Process Efficiency:**
- Time from RFI² publication to contract award (target: 9-12 months)
- Cost of process (staff time, vendor PoC payments)
- Vendor satisfaction with process (survey)

**Innovation:**
- How novel was winning solution vs. current capabilities?
- Adoption by other states or agencies (replicability)

---

*This completes the core templates and frameworks. Additional templates for Vendor Evaluation Matrix and Project Prioritization Scorecard can be provided as needed.*